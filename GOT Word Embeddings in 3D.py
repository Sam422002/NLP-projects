# -*- coding: utf-8 -*-
"""GOT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eMiHPU7W9Kr-vbqkuga_raxmQ0-f8sfs
"""



# 📦 Install the Gensim library for topic modeling and word embedding models like Word2Vec, FastText, etc.
!pip install gensim

# 📚 Import Gensim - library for topic modeling and word embeddings (e.g., Word2Vec, LDA)
import gensim

# 🐼 Import pandas - for data manipulation and handling structured datasets
import pandas as pd

# 📖 Import sentence tokenizer from NLTK to split text into individual sentences
from nltk import sent_tokenize

# 🧹 Import Gensim's text preprocessing utility to lowercase, tokenize, and remove punctuation from text
from gensim.utils import simple_preprocess



# 🚫 Import predefined list of English stopwords (common words like 'the', 'and', etc.) from Gensim
from gensim.parsing.preprocessing import STOPWORDS

# 🧾 Display the set of stopwords
STOPWORDS

# 📖 Import NLTK's sentence tokenizer
from nltk.tokenize import sent_tokenize

# 🧹 Import Gensim's text cleaner - lowercases, tokenizes, removes punctuation, numbers, etc.
from gensim.utils import simple_preprocess

# 📁 List of file paths containing text data (e.g., Game of Thrones scripts)
file_paths = [
    "/content/001ssb.txt",
    "/content/002ssb.txt",
    "/content/003ssb.txt",
    "/content/004ssb.txt",
    "/content/005ssb.txt"
]

# 📚 Load the content of all text files into a list
all_texts = []
for path in file_paths:
    with open(path, 'r', encoding='cp1252') as f:  # cp1252 encoding for special characters
        all_texts.append(f.read())

# 🧽 Preprocessing: Sentence tokenization + word tokenization + stopword removal
cleaned_sentences = []
for text in all_texts:
    sentences = sent_tokenize(text)  # Break full text into sentences
    for sentence in sentences:
        tokens = simple_preprocess(sentence)  # Tokenize each sentence into lowercase words
        clean_tokens = [word for word in tokens if word not in STOPWORDS]  # Remove stopwords
        if clean_tokens:  # Ignore empty or meaningless lines
            cleaned_sentences.append(clean_tokens)

# 🧠 Train a Word2Vec model on the cleaned and tokenized sentences
model = gensim.models.Word2Vec(
    sentences=cleaned_sentences,  # List of tokenized sentences
    vector_size=100,              # Size of word vectors (default = 100)
    window=10,                    # Context window size
    min_count=2,                  # Ignores words that appear fewer than 2 times
    workers=4,                    # Use 4 CPU cores for faster training
    sg=1,                         # 1 = Skip-gram; 0 = CBOW
    epochs=25                     # Number of training iterations
)

# 🔧 Builds the vocabulary (dictionary of unique words and counts) from the tokenized sentences
model.build_vocab(cleaned_sentences)

model.train(cleaned_sentences , total_examples = model.corpus_count , epochs = 25)

model.wv.most_similar('daenerys')  # 🔍 Finds the top 10 words most similar to 'daenerys' based on cosine similarity from the trained Word2Vec model

# Run the 'doesnt_match' function on a list of words related to Game of Thrones characters
model.wv.doesnt_match(['jon', 'rikon', 'robb', 'arya', 'sansa', 'bran'])  # 🔍 Finds the word with the least similarity to the others

# Get the shape of the word vector for the word 'jon'
model.wv['jon'].shape  # 🔍 Returns the shape of the vector representing 'jon', e.g., (100,) if using a 100-dimensional vector

# Calculate the cosine similarity between the word vectors of 'arya' and 'sansa'
model.wv.similarity('arya', 'sansa')  # 🔍 Returns a value between -1 and 1, representing how similar 'arya' and 'sansa' are based on their vectors

# Calculate the cosine similarity between the word vectors of 'arya' and 'cersei'
model.wv.similarity('arya', 'cersei')  # 🔍 Returns a value between -1 and 1, showing how similar 'arya' and 'cersei' are in the embedding space

model.wv.similarity('sansa','cersei')

model.wv.similarity('sansa','tywin')

# Get the most similar words to 'winter' based on the trained Word2Vec model
model.wv.most_similar('winter')  # 🔍 Returns the top 10 words most similar to 'winter' based on cosine similarity between their word vectors

# Get the normalized word vectors (unit vectors) for all words in the model's vocabulary
model.wv.get_normed_vectors()  # 🔍 Returns the word vectors after normalizing them to have unit length (magnitude = 1)

# Get the list of words (vocabulary) in the model's index, ordered by frequency of occurrence
y = model.wv.index_to_key  # 🔍 Returns a list of words in the model's vocabulary (indexed in order of frequency)

len(y)

from sklearn.decomposition import PCA

pca = PCA(n_components=3)

y

model.wv.get_normed_vectors()

X = pca.fit_transform(model.wv.get_normed_vectors())
X[:5]

import plotly.express as px

# Create a 3D scatter plot using Plotly, displaying a subset of the word vectors (X[200:300]) with 3D coordinates
# and color-coded by the corresponding words in the vocabulary (y[200:300])

fig = px.scatter_3d(X[200:300], x=0, y=1, z=2, color=y[200:300])  # 🔍 Visualize the word embeddings in 3D space, where X contains the word vectors
fig.show()  # 🔍 Display the 3D plot

from sklearn.manifold import TSNE
tsne = TSNE(n_components=2, random_state=42)
X_2d = tsne.fit_transform(model.wv.get_normed_vectors())

# Get words for labeling (you can limit for clarity)
words = model.wv.index_to_key[:300]  # First 300 most frequent words

# Only take the corresponding 2D points
X_subset = X_2d[:300]

# Plot
fig = px.scatter(
    x=X_subset[:, 0], y=X_subset[:, 1],
    text=words,
    title="t-SNE Visualization of Word2Vec Embeddings (2D)",
    labels={'x': 't-SNE 1', 'y': 't-SNE 2'}
)
fig.update_traces(textposition='top center')
fig.show()

model.wv.most_similar(positive = ['king','woman'] , negative = ['man'])

































